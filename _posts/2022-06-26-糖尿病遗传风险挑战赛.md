# 基于集成学习的糖尿病遗传风险挑战赛
[赛题网址](http://challenge.xfyun.cn/topic/info?type=diabetes&option=ssgy)


## 代码

### 导入需要的packages
```
#import libraries
from os import path

# data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd

#Visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import seaborn as sns

#Configure Visualization Defaults
#%matplotlib inline = show plots in Jupyter Notebook browser
%matplotlib inline
mpl.style.use('ggplot')
pylab.rcParams['figure.figsize'] = 12,8

import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls

# machine learning
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, 
                              GradientBoostingClassifier, ExtraTreesClassifier)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process

from xgboost import XGBClassifier
import xgboost as xgb


#Common Model Helpers
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn import feature_selection
from sklearn import model_selection
from sklearn import metrics
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
from sklearn.metrics import classification_report, balanced_accuracy_score, f1_score

#stacking
from sklearn.model_selection import KFold

#hyperparameter tuning
from hyperopt import STATUS_OK, Trials, fmin, hp, tpe

import warnings
warnings.filterwarnings('ignore')
```
### 读取数据，查看数据字段
```
# 导入训练集数据并查看前5行
train_df = pd.read_csv("E:/学习/2021-2022学年学习文件/研一下课程文件/Python编程项目/糖尿病遗传风险预测挑战赛公开数据/比赛训练集.csv",engine='python',encoding='gbk')
train_df.head()
# 导入测试集数据并查看前5行
test_df = pd.read_csv("E:/学习/2021-2022学年学习文件/研一下课程文件/Python编程项目/糖尿病遗传风险预测挑战赛公开数据/比赛测试集.csv",engine='python',encoding='gbk')
test_df.head()

train_df.columns
test_df.columns
```

### 数据预处理

```
#缺失值统计
def missing_value_summary(dataframe):
    """返回每一列缺失值对应的比例"""
    return dataframe.isna().sum() / dataframe.shape[0] * 100

missing_value_summary(train_df)

#缺失值统计
missing_value_summary(test_df)

#结合变量实际含义分析字段类型
train_df.head()


```
数值型变量：体重指数，舒张压，口服耐糖量测试，胰岛素释放实验，肱三头肌皮褶厚度，出生年份。

类别型变量：性别，糖尿病家族史，患有糖尿病标识。

```
#缺失值处理：使用均值填充
train_df['舒张压'] =  train_df['舒张压'].fillna(train_df['舒张压'].mean())
test_df['舒张压'] =  test_df['舒张压'].fillna(test_df['舒张压'].mean())
missing_value_summary(train_df)
missing_value_summary(test_df)
```

### 特征工程
```
#将“出生年份”转化为“年龄”
train_df['年龄'] = 2022 - train_df['出生年份']
test_df['年龄'] = 2022 - test_df['出生年份']

#删去“编号”列和“出生年份”列
train_df = train_df.drop(['编号','出生年份'],axis=1)
test_df = test_df.drop(['编号','出生年份'],axis=1)

#把“性别”转化为“object"
train_df['性别'] = train_df['性别'].astype('category')
test_df['性别'] = test_df['性别'].astype('category')
```

```
count=pd.DataFrame(train_df['性别'].value_counts())
print(count)

count=pd.DataFrame(train_df['糖尿病家族史'].value_counts())
print(count)

dict_糖尿病家族史 = {
    '无记录': 0,
    '叔叔或姑姑有一方患有糖尿病': 1,
    '叔叔或者姑姑有一方患有糖尿病': 1,
    '父母有一方患有糖尿病': 2
}

train_df['糖尿病家族史'] = train_df['糖尿病家族史'].map(dict_糖尿病家族史)
test_df['糖尿病家族史'] = test_df['糖尿病家族史'].map(dict_糖尿病家族史)


from sklearn.preprocessing import OneHotEncoder
train_df=pd.get_dummies(train_df,columns=['性别'])
train_df=pd.get_dummies(train_df,columns=['糖尿病家族史'])
test_df=pd.get_dummies(test_df,columns=['性别'])
test_df=pd.get_dummies(test_df,columns=['糖尿病家族史'])
```

```
train_df = train_df.drop(['性别_0','糖尿病家族史_0'],axis=1)
test_df = test_df.drop(['性别_0','糖尿病家族史_0'],axis=1)

def BMI(a):
    if a<18.5:
        return 0
    elif 18.5<=a<=24:
        return 1
    elif 24<a<=27:
        return 2
    elif 27<a<=32:
        return 3
    else:
        return 4
    
train_df['BMI']=train_df['体重指数'].apply(BMI)
test_df['BMI']=test_df['体重指数'].apply(BMI)

def DBP(a):
    if a<60:
        return 0
    elif 60<=a<=90:
        return 1
    elif a>90:
        return 2
    else:
        return a
train_df['DBP']=train_df['舒张压'].apply(DBP)
test_df['DBP']=test_df['舒张压'].apply(DBP)

train_df.columns
test_df.columns
```

### 使用集成方法进行建模分析

#### 定义函数

```
X_train = train_df.iloc[:,[0,1,2,3,4,6,7,8,9,10,11]]
Y_train = train_df.iloc[:,[5]]
test = test_df

```



```
# Some useful parameters which will come in handy later on
ntrain = train_df.shape[0]
ntest = test_df.shape[0]
SEED = 0 # for reproducibility
NFOLDS = 5 # set folds for out-of-fold prediction
kf = KFold(n_splits= NFOLDS)

# Class to extend the Sklearn classifier
class SklearnHelper(object):
    def __init__(self, clf, params=None):
#        params['random_state'] = seed
        self.clf = clf(**params)

    def train(self, x_train, y_train):
        self.clf.fit(x_train, y_train)

    def predict(self, x):
        return self.clf.predict(x)
    
    def fit(self,x,y):
        return self.clf.fit(x,y)
    
    def feature_importances(self,x,y):
        print(self.clf.fit(x,y).feature_importances_)
```

```
#function to get the out of fold predictions for each of the base models
def get_oof(clf, x_train, y_train, x_test):
    oof_train = np.zeros((ntrain,))
    oof_test = np.zeros((ntest,))
    oof_test_skf = np.empty((NFOLDS, ntest))

    for i, (train_index, test_index) in enumerate(kf.split(x_train)):
        x_tr = x_train[train_index]
        y_tr = y_train[train_index]
        x_te = x_train[test_index]

        clf.train(x_tr, y_tr)

        oof_train[test_index] = clf.predict(x_te)
        oof_test_skf[i, :] = clf.predict(x_test)

    oof_test[:] = oof_test_skf.mean(axis=0)
    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)
```

#### Logistic Regression CV

```
#create lists of values for hyperparameters to tune using param_grid
grid_C = [100, 10, 1.0, 0.1, 0.01]

#split dataset in cross-validation with this splitter class: 
cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%

#base model
LR = linear_model.LogisticRegressionCV()
base_results = model_selection.cross_validate(LR, X_train, Y_train, cv  = cv_split, return_train_score=True)
LR.fit(X_train, Y_train)

print('BEFORE LR Parameters: ', LR.get_params())
print("BEFORE LR Training w/bin score mean: {:.2f}". format(base_results['train_score'].mean()*100)) 
print("BEFORE LR Test w/bin score mean: {:.2f}". format(base_results['test_score'].mean()*100))
print("BEFORE LR Test w/bin score 3*std: +/- {:.2f}". format(base_results['test_score'].std()*100*3))
print('-'*10)


#tune hyper-parameters:
param_grid = {'Cs': grid_C,  
              'penalty': ['none', 'l1', 'l2', 'elasticnet'], 
              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
           }

#choose best model with grid_search: 
tune_model = model_selection.GridSearchCV(LR, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)
tune_model.fit(X_train, Y_train)

print('AFTER LR Parameters: ', tune_model.best_params_)
print("AFTER LR Training w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) 
print("AFTER LR Test w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))
print("AFTER LR Test w/bin score 3*std: +/- {:.2f}". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))
print('-'*10)

```

#### K Neighbours Classifier

```
#create lists of values for hyperparameters to tune using param_grid
grid_neighbors = [1,5,9,13,17,21,30,50]

#split dataset in cross-validation with this splitter class:
cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%

#base model
KN = neighbors.KNeighborsClassifier()
base_results = model_selection.cross_validate(KN, X_train, Y_train, cv  = cv_split, return_train_score=True)
KN.fit(X_train, Y_train)

print('BEFORE KN Parameters: ', KN.get_params())
print("BEFORE KN Training w/bin score mean: {:.2f}". format(base_results['train_score'].mean()*100)) 
print("BEFORE KN Test w/bin score mean: {:.2f}". format(base_results['test_score'].mean()*100))
print("BEFORE KN Test w/bin score 3*std: +/- {:.2f}". format(base_results['test_score'].std()*100*3))
print('-'*10)


#tune hyper-parameters:
param_grid = {'n_neighbors': grid_neighbors  
           }

#choose best model with grid_search

tune_model = model_selection.GridSearchCV(KN, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)
tune_model.fit(X_train, Y_train)

print('AFTER KN Parameters: ', tune_model.best_params_)
print("AFTER KN Training w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) 
print("AFTER KN Test w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))
print("AFTER KN Test w/bin score 3*std: +/- {:.2f}". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))

```

#### Random Forest
```
#split training data into training and validation set
X_training, X_val, y_training, y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=0)

#function to explore individual hyperparameters so that final param grid can be more finely tuned to save computation time
def explore_RF_hyperparameters(hyperparameter, hyperparameter_values, scoring='accuracy'):

    param_grid = [{hyperparameter: hyperparameter_value} for hyperparameter_value in hyperparameter_values]
    score_training_data = []
    score_val_data = []

    for i, _ in enumerate(hyperparameter_values):
        clf = RandomForestClassifier(random_state=0, **param_grid[i])
        clf.fit(X_training, y_training)
        scorer = metrics.get_scorer(scoring)
        score_training_data.append(scorer(clf, X_training, y_training))
        score_val_data.append(scorer(clf, X_val, y_val))

    plt.plot(hyperparameter_values, score_training_data)
    plt.plot(hyperparameter_values, score_val_data)
    plt.legend(['Training Data', 'Validation Data'])
    plt.xlabel('Hyperparameter Value')
    plt.ylabel('Score')
    plt.show()

#varying the number of estimators   
explore_RF_hyperparameters(
    hyperparameter="n_estimators",
    hyperparameter_values = [1, 10, 25, 50, 75, 100, 150]
)

#varying the max depth 
explore_RF_hyperparameters(
    hyperparameter="max_depth",
    hyperparameter_values = [2, 4, 6, 8, 10, 12, 14, 16 , 20, 30]
)

#varying the min samples per split
explore_RF_hyperparameters(
    hyperparameter="min_samples_split",
    hyperparameter_values = [ 2, 3, 4, 5,10,20]
)

#varying the min samples per leaf
explore_RF_hyperparameters(
    hyperparameter="min_samples_leaf",
    hyperparameter_values = [1, 2, 3, 4, 5]
)


#create lists of values for hyperparameters to tune using param_grid
grid_n_estimator = [ 10, 20, 30]
grid_max_depth = [ 8, 10, 12]
grid_seed = [0]
grid_split = [5,10,15]
grid_leaf = [2,3]

#split dataset in cross-validation with this splitter class:
cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%

#base model
RF = ensemble.RandomForestClassifier()
base_results = model_selection.cross_validate(RF, X_train, Y_train, cv  = cv_split, return_train_score=True)
RF.fit(X_train, Y_train)

print('BEFORE RF Parameters: ', RF.get_params())
print("BEFORE RF Training w/bin score mean: {:.2f}". format(base_results['train_score'].mean()*100)) 
print("BEFORE RF Test w/bin score mean: {:.2f}". format(base_results['test_score'].mean()*100))
print("BEFORE RF Test w/bin score 3*std: +/- {:.2f}". format(base_results['test_score'].std()*100*3))
print('-'*10)

#tune hyper-parameters:
param_grid = {'n_estimators': grid_n_estimator,  
              'criterion': ['gini', 'entropy'], 
              'max_depth': grid_max_depth,    
              'random_state': grid_seed,
              'min_samples_split': grid_split,
              'min_samples_leaf': grid_leaf,
              'max_features': ['auto', 'sqrt', 'log2']
           }


#choose best model with grid_search: 
tune_model = model_selection.GridSearchCV(RF, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)
tune_model.fit(X_train, Y_train)

print('AFTER RF Parameters: ', tune_model.best_params_)
print("AFTER RF Training w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) 
print("AFTER RF Test w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))
print("AFTER RF Test w/bin score 3*std: +/- {:.2f}". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))
print('-'*10)
```

#### Extra Trees

```
#function to explore individual hyperparameters so that final param grid can be more finely tuned to save computation time
def explore_ET_hyperparameters(hyperparameter, hyperparameter_values, scoring='accuracy'):

    param_grid = [{hyperparameter: hyperparameter_value} for hyperparameter_value in hyperparameter_values]
    score_training_data = []
    score_val_data = []

    for i, _ in enumerate(hyperparameter_values):
        clf = ExtraTreesClassifier(random_state=0, **param_grid[i])
        clf.fit(X_training, y_training)
        scorer = metrics.get_scorer(scoring)
        score_training_data.append(scorer(clf, X_training, y_training))
        score_val_data.append(scorer(clf, X_val, y_val))

    plt.plot(hyperparameter_values, score_training_data)
    plt.plot(hyperparameter_values, score_val_data)
    plt.legend(['Training Data', 'Validation Data'])
    plt.xlabel('Hyperparameter Value')
    plt.ylabel('Score')
    plt.show()

#varying the number of estimators   
explore_ET_hyperparameters(
    hyperparameter="n_estimators",
    hyperparameter_values = [1,3,6, 10, 20,30]
)



#varying the min samples per split
explore_ET_hyperparameters(
    hyperparameter="min_samples_split",
    hyperparameter_values = [ 2, 3, 4, 5,10,20]
)

#varying the min samples per leaf
explore_ET_hyperparameters(
    hyperparameter="min_samples_leaf",
    hyperparameter_values = [1, 2, 3, 4, 5]
)

#varying the max depth 
explore_ET_hyperparameters(
    hyperparameter="max_depth",
    hyperparameter_values = [2, 4, 6, 8, 10, 12, 14, 16 , 20, 30]
)

#create lists of values for hyperparameters to tune using param_grid
grid_n_estimator = [ 6, 10, 14]
grid_max_depth = [ 8, 10, 12]
grid_seed = [0]
grid_split = [5,10,15]
grid_leaf = [2,3,4]

#split dataset in cross-validation with this splitter class:
cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%

#base model
ET = ensemble.ExtraTreesClassifier()
base_results = model_selection.cross_validate(ET, X_train, Y_train, cv  = cv_split, return_train_score=True)
ET.fit(X_train, Y_train)

print('BEFORE ET Parameters: ', ET.get_params())
print("BEFORE ET Training w/bin score mean: {:.2f}". format(base_results['train_score'].mean()*100)) 
print("BEFORE ET Test w/bin score mean: {:.2f}". format(base_results['test_score'].mean()*100))
print("BEFORE ET Test w/bin score 3*std: +/- {:.2f}". format(base_results['test_score'].std()*100*3))
print('-'*10)

#tune hyper-parameters:
param_grid = {'n_estimators': grid_n_estimator,  
              'criterion': ['gini', 'entropy'], 
              'max_depth': grid_max_depth,    
              'random_state': grid_seed,
              'min_samples_split': grid_split,
              'min_samples_leaf': grid_leaf,
              'max_features': ['auto', 'sqrt', 'log2']
           }

#choose best model with grid_search: 
tune_model = model_selection.GridSearchCV(ET, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)
tune_model.fit(X_train, Y_train)

print('AFTER ET Parameters: ', tune_model.best_params_)
print("AFTER ET Training w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) 
print("AFTER ET Test w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))
print("AFTER ET Test w/bin score 3*std: +/- {:.2f}". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))
print('-'*10)
```

#### AdaBoost
```
#function to explore individual hyperparameters so that final param grid can be more finely tuned to save computation time
def explore_ADA_hyperparameters(hyperparameter, hyperparameter_values, scoring='accuracy'):

    param_grid = [{hyperparameter: hyperparameter_value} for hyperparameter_value in hyperparameter_values]
    score_training_data = []
    score_val_data = []

    for i, _ in enumerate(hyperparameter_values):
        clf = AdaBoostClassifier(random_state=0, **param_grid[i])
        clf.fit(X_training, y_training)
        scorer = metrics.get_scorer(scoring)
        score_training_data.append(scorer(clf, X_training, y_training))
        score_val_data.append(scorer(clf, X_val, y_val))

    plt.plot(hyperparameter_values, score_training_data)
    plt.plot(hyperparameter_values, score_val_data)
    plt.legend(['Training Data', 'Validation Data'])
    plt.xlabel('Hyperparameter Value')
    plt.ylabel('Score')
    plt.show()

#varying the number of estimators   
explore_ADA_hyperparameters(
    hyperparameter="n_estimators",
    hyperparameter_values = [3,6,10,15,20]
)


#varying the learning rate   
explore_ADA_hyperparameters(
    hyperparameter="learning_rate",
    hyperparameter_values = [.01, .03, .05, .1, .15, .25, 0.35, 0.5]
)

#create lists of values for hyperparameters to tune using param_grid
grid_n_estimator = [ 6, 8, 10, 12]
grid_learn = [ 0.25, 0.3, 0.35, 0.4]

#split dataset in cross-validation with this splitter class:
cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%

#base model
ADA = ensemble.AdaBoostClassifier()
base_results = model_selection.cross_validate(ADA, X_train, Y_train, cv  = cv_split, return_train_score=True)
ADA.fit(X_train, Y_train)

print('BEFORE ADA Parameters: ', ADA.get_params())
print("BEFORE ADA Training w/bin score mean: {:.2f}". format(base_results['train_score'].mean()*100)) 
print("BEFORE ADA Test w/bin score mean: {:.2f}". format(base_results['test_score'].mean()*100))
print("BEFORE ADA Test w/bin score 3*std: +/- {:.2f}". format(base_results['test_score'].std()*100*3))
print('-'*10)


#tune hyper-parameters:
param_grid = {'n_estimators': grid_n_estimator,  
              'learning_rate': grid_learn
           }

#choose best model with grid_search:
tune_model = model_selection.GridSearchCV(ADA, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score=True)
tune_model.fit(X_train, Y_train)

print('AFTER ADA Parameters: ', tune_model.best_params_)
print("AFTER ADA Training w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) 
print("AFTER ADA Test w/bin score mean: {:.2f}". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))
print("AFTER ADA Test w/bin score 3*std: +/- {:.2f}". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))
print('-'*10)
```

#### Create Ensemble

```
# Put in our parameters for said classifiers
# Random Forest parameters
rf_params = {
    'n_estimators': 30,
    'max_depth': 12,
    'min_samples_leaf': 2,
    'min_samples_split' : 15,
    'random_state' : 0,
    'criterion' : 'entropy',
    'max_features' : 'auto'
}

# Extra Trees Parameters
et_params = {
    'n_jobs': -1,
    'n_estimators': 14,
    'max_features': 'auto',
    'random_state' : 0,    
    'max_depth': 12,
    'min_samples_leaf': 2,
    'min_samples_split' : 10,
    'criterion': 'entropy'
}

# AdaBoost parameters
ada_params = {
    'n_estimators': 12,
    'learning_rate' : 0.4
}

# Gradient Boosting parameters
gb_params = {
    'n_estimators': 150,
    'max_depth': 4,
    'learning_rate': 0.05,
    'criterion' : 'mse',
    'loss' : 'exponential'
}



# LogisticRegressionCV parameters 
lr_params = {
    'solver' : 'newton-cg',
    'penalty' : 'l2',
    'Cs' : 10
    }

# K Neighbours Classifier parameters 
kn_params = {
    'n_neighbors' : 21
    }

```

```
# Create objects that represent our models
rf = SklearnHelper(clf=RandomForestClassifier, params=rf_params)
et = SklearnHelper(clf=ExtraTreesClassifier, params=et_params)
ada = SklearnHelper(clf=AdaBoostClassifier, params=ada_params)
gb = SklearnHelper(clf=GradientBoostingClassifier, params=gb_params)
kn = SklearnHelper(clf=KNeighborsClassifier, params=kn_params)
lr = SklearnHelper(clf=LogisticRegressionCV, params=lr_params)

```

```
x_bin = ['体重指数', '舒张压', '口服耐糖量测试', '胰岛素释放实验', '肱三头肌皮褶厚度', '年龄', '性别_1',
       '糖尿病家族史_1', '糖尿病家族史_2', 'BMI', 'DBP']

# Create Numpy arrays of train, test and target dataframes to feed into our models
y_train = train_df['患有糖尿病标识'].ravel()
x_train = train_df[x_bin].values # Creates an array of the train data
x_test = test_df[x_bin].values # Creats an array of the test data
```

```
# Create our OOF train and test predictions. These base results will be used as new features
et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees
rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest
ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost 
gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost
kn_oof_train, kn_oof_test = get_oof(kn,x_train, y_train, x_test) # K Neighbours Classifier
lr_oof_train, lr_oof_test = get_oof(lr,x_train, y_train, x_test) # Support Vector Classifier

print("Training is complete")

```

```
#calculate feature importances
rf_feature = rf.feature_importances(x_train,y_train)
#et_feature = et.feature_importances(x_train, y_train)
ada_feature = ada.feature_importances(x_train, y_train)
gb_feature = gb.feature_importances(x_train,y_train)

```

```
#'KNN' and 'LR' object has no attribute 'feature_importances_' so cant do this for it
rf_features = [0.32788851, 0.12135376, 0.15327155, 0.08130874, 0.25204181, 0.02267604,
 0.00384155, 0.0028458,  0.0039967,  0.02331319, 0.00746236]
ada_features = [0.08333333, 0.33333333, 0.25,       0.25,       0.08333333, 0.,
 0.,         0.,         0.,         0.,         0.        ]
gb_features = [3.03786894e-01, 1.49413571e-01, 1.64210218e-01, 1.09988946e-01,
 2.43078460e-01, 2.53030809e-02, 1.18358968e-03, 4.03770943e-04,
 1.77514871e-03, 8.10548775e-04, 4.57715855e-05]

```

```
# Create a dataframe with features
feature_dataframe = pd.DataFrame( {'features': x_bin,
     'Random Forest feature importances': rf_features,
     #'Extra Trees  feature importances': et_features,
     'AdaBoost feature importances': ada_features,
     'Gradient Boost feature importances': gb_features
    })
```

```
# Scatter plot of feature importances
trace = go.Scatter(
    y = feature_dataframe['Random Forest feature importances'].values,
    x = feature_dataframe['features'].values,
    mode='markers',
    marker=dict(
        sizemode = 'diameter',
        sizeref = 1,
        size = 25,
#       size= feature_dataframe['AdaBoost feature importances'].values,
        #color = np.random.randn(500), #set color equal to a variable
        color = feature_dataframe['Random Forest feature importances'].values,
        colorscale='Portland',
        showscale=True
    ),
    text = feature_dataframe['features'].values
)
data = [trace]

layout= go.Layout(
    autosize= True,
    title= 'Random Forest Feature Importance',
    hovermode= 'closest',
#     xaxis= dict(
#         title= 'Pop',
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig,filename='scatter2010')
```

```
# Scatter plot 
trace = go.Scatter(
    y = feature_dataframe['AdaBoost feature importances'].values,
    x = feature_dataframe['features'].values,
    mode='markers',
    marker=dict(
        sizemode = 'diameter',
        sizeref = 1,
        size = 25,
#       size= feature_dataframe['AdaBoost feature importances'].values,
        #color = np.random.randn(500), #set color equal to a variable
        color = feature_dataframe['AdaBoost feature importances'].values,
        colorscale='Portland',
        showscale=True
    ),
    text = feature_dataframe['features'].values
)
data = [trace]

layout= go.Layout(
    autosize= True,
    title= 'AdaBoost Feature Importance',
    hovermode= 'closest',
#     xaxis= dict(
#         title= 'Pop',
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig,filename='scatter2010')
```


```
# Scatter plot 
trace = go.Scatter(
    y = feature_dataframe['Gradient Boost feature importances'].values,
    x = feature_dataframe['features'].values,
    mode='markers',
    marker=dict(
        sizemode = 'diameter',
        sizeref = 1,
        size = 25,
#       size= feature_dataframe['AdaBoost feature importances'].values,
        #color = np.random.randn(500), #set color equal to a variable
        color = feature_dataframe['Gradient Boost feature importances'].values,
        colorscale='Portland',
        showscale=True
    ),
    text = feature_dataframe['features'].values
)
data = [trace]

layout= go.Layout(
    autosize= True,
    title= 'Gradient Boosting Feature Importance',
    hovermode= 'closest',
#     xaxis= dict(
#         title= 'Pop',
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig,filename='scatter2010')
```

```
# Create the new column containing the average of values
feature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise
feature_dataframe.head(3)
```

```
#a plot of the average feature importances across the models
y = feature_dataframe['mean'].values
x = feature_dataframe['features'].values
data = [go.Bar(
            x= x,
             y= y,
            width = 0.5,
            marker=dict(
               color = feature_dataframe['mean'].values,
            colorscale='Portland',
            showscale=True,
            reversescale = False
            ),
            opacity=0.6
        )]

layout= go.Layout(
    autosize= True,
    title= 'Barplots of Mean Feature Importance',
    hovermode= 'closest',
#     xaxis= dict(
#         title= 'Pop',
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig, filename='bar-direct-labels')
```

```
#create base predictions
base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),
     #'ExtraTrees': et_oof_train.ravel(),
     'AdaBoost': ada_oof_train.ravel(),
     'GradientBoost': gb_oof_train.ravel(),
     'KNeighbours': kn_oof_train.ravel(),
     'LogisticRegression': lr_oof_train.ravel()
    })
base_predictions_train.head()

```

```
#plot correlation between the models
data = [
    go.Heatmap(
        z= base_predictions_train.astype(float).corr().values ,
        x=base_predictions_train.columns.values,
        y= base_predictions_train.columns.values,
          colorscale='Viridis',
            showscale=True,
            reversescale = True
    )
]
py.iplot(data, filename='labelled-heatmap')

```

```
#create new train and test sets with the guesses from our ensemble
x_train_ensemble = np.concatenate(( lr_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, kn_oof_train), axis=1)
x_test_ensemble = np.concatenate(( lr_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, kn_oof_test), axis=1)
```


#### Tune Hyperparameters for XGBoost

```
X_training_ensemble, X_val_ensemble, y_training, y_val = train_test_split(x_train_ensemble, y_train, test_size=0.3, random_state=42)
```

```
#function to explore individual hyperparameters so that final param grid can be more finely tuned to save computation time
def explore_XGB_hyperparameters(hyperparameter, hyperparameter_values, scoring='accuracy'):

    param_grid = [{hyperparameter: hyperparameter_value} for hyperparameter_value in hyperparameter_values]
    score_training_data = []
    score_val_data = []

    for i, _ in enumerate(hyperparameter_values):
        clf = XGBClassifier(random_state=0, **param_grid[i])
        clf.fit(X_training_ensemble, y_training)
        scorer = metrics.get_scorer(scoring)
        score_training_data.append(scorer(clf, X_training_ensemble, y_training))
        score_val_data.append(scorer(clf, X_val_ensemble, y_val))

    plt.plot(hyperparameter_values, score_training_data)
    plt.plot(hyperparameter_values, score_val_data)
    plt.legend(['Training Data', 'Validation Data'])
    plt.xlabel('Hyperparameter Value')
    plt.ylabel('Score')
    plt.show()

#varying gamma 
explore_XGB_hyperparameters(
    hyperparameter="gamma",
    hyperparameter_values = [0.1,1,10,20,50,100]
)
```

```
#varying max depth 
explore_XGB_hyperparameters(
    hyperparameter="max_depth",
    hyperparameter_values = [1,10,20,50]
)


#varying reg_alpa 
explore_XGB_hyperparameters(
    hyperparameter="reg_alpha",
    hyperparameter_values = [1,10,20,50,100]
)

#varying colsample_bytree 
explore_XGB_hyperparameters(
    hyperparameter="colsample_bytree",
    hyperparameter_values = [0,0.1,0.5,1]
)

#varying min_child_weight 
explore_XGB_hyperparameters(
    hyperparameter="min_child_weight",
    hyperparameter_values = [0,5,10,20,35,50,100]
)

#varying n_estimators 
explore_XGB_hyperparameters(
    hyperparameter="n_estimators",
    hyperparameter_values = [5,10,20,50,100]
)

#varying learning_rate 
explore_XGB_hyperparameters(
    hyperparameter="learning_rate",
    hyperparameter_values = [0,0.001,0.01,0.03,0.05,0.1,0.15,0.2,1]
)

#varying subsample 
explore_XGB_hyperparameters(
    hyperparameter="subsample",
    hyperparameter_values = [0,0.005,0.05, 0.1,0.3,0.5,0.7,1]
)
```

```
#create hyperparameter space for the values during tuning to be selected from
space={'max_depth': hp.quniform("max_depth", 1, 10, 1),
        'gamma': hp.uniform ('gamma', 20,80),
        'reg_alpha' : hp.quniform('reg_alpha', 1,50,1),
        'reg_lambda' : hp.uniform('reg_lambda', 0,1),
        'colsample_bytree' : hp.uniform('colsample_bytree', 0,0.5),
        'min_child_weight' : hp.quniform('min_child_weight', 15, 60, 1),
        'n_estimators': hp.quniform('n_estimators', 20, 50, 1),
        'learning_rate': hp.uniform('learning_rate', 0, 1),
        'subsample' : hp.uniform('subsample', 0,1),
        'seed': 0
    }

```

```
#function to run XGBClassifier with hyperparameter values from 'space' and report the accuracy
def objective(space):
    clf=xgb.XGBClassifier(
                    n_estimators =int(space['n_estimators']), max_depth = int(space['max_depth']), gamma = space['gamma'],
                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),
                    colsample_bytree=int(space['colsample_bytree']))
    
    evaluation = [( X_training_ensemble, y_training), ( X_val_ensemble, y_val)]
    
    clf.fit(X_training_ensemble, y_training,
            eval_set=evaluation, eval_metric="auc",
            early_stopping_rounds=10,verbose=False)
    

    pred = clf.predict(X_val_ensemble)
    accuracy = accuracy_score(y_val, pred>0.5)
    print (accuracy)
    return {'loss': -accuracy, 'status': STATUS_OK }
```

```
#run a large number of trials constantly varying the hyperparameters and record the best
trials = Trials()

best_hyperparams = fmin(fn = objective,
                        space = space,
                        algo = tpe.suggest,
                        max_evals = 300,
                        trials = trials)

print("The best hyperparameters are : ","\n")
print(best_hyperparams)

```

#### Submit Result

```
#ensemble stacking method
gbm = xgb.XGBClassifier(
 learning_rate = 0.15749877569136,
 n_estimators= 50,
 max_depth= 8,
 min_child_weight= 44,
 gamma=79.86233141368602,                        
 subsample=0.4893908606299252,
 colsample_bytree=0.06392142496161599,
 #objective= 'binary:logistic',
 nthread= -1,
 reg_alpha=14,
 reg_lambda=0.07394511128684644
 #scale_pos_weight=1
).fit(x_train_ensemble, y_train)

predictions = gbm.predict(x_test_ensemble)
#Generate Submission File 
predict_y = pd.DataFrame(predictions)
predict_y.to_csv(path_or_buf=r'E:/学习/2021-2022学年学习文件/研一下课程文件/Python编程项目/糖尿病遗传风险预测挑战赛公开数据/预测结果.csv')

```
